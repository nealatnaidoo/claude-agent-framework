# Canonical GitLab CI/CD Template: Databricks Jobs & Notebooks
# Version: 1.0
# Last Updated: 2026-02-04
# Decision Reference: DEC-DEVOPS-019
#
# This template deploys Databricks notebooks, jobs, and Python wheels.
# Separate from application CI/CD due to different lifecycle.
#
# Project Structure Expected:
# ├── notebooks/
# │   ├── bronze/
# │   ├── silver/
# │   └── gold/
# ├── src/                     # Python library code
# │   └── mypackage/
# ├── jobs/                    # Job definitions (JSON/YAML)
# │   ├── daily_etl.yaml
# │   └── weekly_report.yaml
# ├── tests/
# └── pyproject.toml
#
# Lessons Applied: 88-95 (Databricks patterns)

stages:
  - quality
  - test
  - build
  - deploy-dev
  - deploy-staging
  - deploy-prod

variables:
  PYTHON_VERSION: "3.10"  # Match Databricks runtime
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"

  # Databricks Workspaces (per environment)
  DATABRICKS_HOST_DEV: "https://adb-dev.azuredatabricks.net"
  DATABRICKS_HOST_STAGING: "https://adb-staging.azuredatabricks.net"
  DATABRICKS_HOST_PROD: "https://adb-prod.azuredatabricks.net"

  # Unity Catalog (Lesson #89: DBFS deprecated, use Volumes)
  CATALOG_DEV: "dev_catalog"
  CATALOG_STAGING: "staging_catalog"
  CATALOG_PROD: "prod_catalog"
  VOLUME_PATH: "/Volumes/${CATALOG}/default/packages"

# ============================================================================
# REUSABLE TEMPLATES
# ============================================================================

.python-base:
  image: python:${PYTHON_VERSION}-slim
  cache:
    key: python-${CI_COMMIT_REF_SLUG}
    paths:
      - .cache/pip
      - .venv
  before_script:
    - python -m venv .venv
    - source .venv/bin/activate
    - pip install --upgrade pip build
    - pip install -e ".[dev]"

.databricks-cli:
  image: python:${PYTHON_VERSION}-slim
  before_script:
    - pip install databricks-cli databricks-sdk
    - |
      # Configure Databricks CLI
      cat > ~/.databrickscfg << EOF
      [DEFAULT]
      host = ${DATABRICKS_HOST}
      token = ${DATABRICKS_TOKEN}
      EOF

# ============================================================================
# STAGE 1: QUALITY GATES (NON-NEGOTIABLE)
# ============================================================================

lint:
  extends: .python-base
  stage: quality
  script:
    - pip install ruff
    - ruff check src/ notebooks/ tests/
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH =~ /^(main|develop)$/

type-check:
  extends: .python-base
  stage: quality
  script:
    - pip install mypy
    - mypy src/ --ignore-missing-imports

# Lesson #88: Check for DBFS usage (deprecated)
dbfs-deprecation-check:
  extends: .python-base
  stage: quality
  script:
    - |
      echo "Checking for deprecated DBFS patterns..."
      DBFS_USAGE=$(grep -r "dbfs:/" notebooks/ src/ --include="*.py" --include="*.sql" | wc -l || echo "0")
      if [ "$DBFS_USAGE" -gt 0 ]; then
        echo "WARNING: Found $DBFS_USAGE uses of dbfs:/ paths"
        grep -r "dbfs:/" notebooks/ src/ --include="*.py" --include="*.sql"
        echo "Consider migrating to Unity Catalog Volumes (/Volumes/...)"
      fi

# Lesson #92: Validate decimal precision
decimal-precision-check:
  extends: .python-base
  stage: quality
  script:
    - |
      echo "Checking for Decimal configurations..."
      # Ensure DecimalType precision matches across codebase
      grep -r "DecimalType\|DECIMAL" notebooks/ src/ --include="*.py" --include="*.sql" || true

# ============================================================================
# STAGE 2: TESTING (NON-NEGOTIABLE)
# ============================================================================

unit-tests:
  extends: .python-base
  stage: test
  script:
    - pip install pytest pytest-cov
    - pytest tests/unit/ -v --junitxml=junit.xml --cov=src --cov-report=xml
  artifacts:
    reports:
      junit: junit.xml
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml

# Lesson #94: Test widget defaults match actual files
widget-validation:
  extends: .python-base
  stage: test
  script:
    - |
      echo "Validating notebook widget defaults..."
      # Extract widget defaults and validate they match expected patterns
      python -c "
      import ast
      import glob

      for nb in glob.glob('notebooks/**/*.py', recursive=True):
          with open(nb) as f:
              content = f.read()
              if 'dbutils.widgets' in content:
                  print(f'Found widgets in {nb}')
      "

# ============================================================================
# STAGE 3: BUILD
# ============================================================================

build-wheel:
  extends: .python-base
  stage: build
  script:
    - python -m build --wheel
    - ls -la dist/
  artifacts:
    paths:
      - dist/*.whl
    expire_in: 1 week
  rules:
    - if: $CI_COMMIT_BRANCH =~ /^(main|develop)$/

# ============================================================================
# STAGE 4: DEPLOY DEV (AUTOMATIC)
# ============================================================================

deploy-dev:
  extends: .databricks-cli
  stage: deploy-dev
  variables:
    DATABRICKS_HOST: $DATABRICKS_HOST_DEV
    DATABRICKS_TOKEN: $DATABRICKS_TOKEN_DEV
    CATALOG: $CATALOG_DEV
  environment:
    name: development
  script:
    - |
      # Upload wheel to Unity Catalog Volume (Lesson #89)
      WHEEL_FILE=$(ls dist/*.whl | head -1)
      WHEEL_NAME=$(basename $WHEEL_FILE)

      echo "Uploading $WHEEL_NAME to ${VOLUME_PATH}/"
      databricks fs cp $WHEEL_FILE ${VOLUME_PATH}/${WHEEL_NAME} --overwrite

      # Sync notebooks
      echo "Syncing notebooks..."
      databricks workspace import-dir notebooks /Repos/dev/${CI_PROJECT_NAME} --overwrite

      # Update/create jobs
      for job_file in jobs/*.yaml; do
        if [ -f "$job_file" ]; then
          JOB_NAME=$(basename $job_file .yaml)
          echo "Deploying job: $JOB_NAME"

          # Check if job exists
          JOB_ID=$(databricks jobs list --output JSON | python -c "
          import json, sys
          jobs = json.load(sys.stdin).get('jobs', [])
          for j in jobs:
              if j['settings']['name'] == '${JOB_NAME}_dev':
                  print(j['job_id'])
                  break
          " || echo "")

          if [ -n "$JOB_ID" ]; then
            echo "Updating existing job $JOB_ID"
            databricks jobs reset --job-id $JOB_ID --json-file $job_file
          else
            echo "Creating new job"
            databricks jobs create --json-file $job_file
          fi
        fi
      done
  rules:
    - if: $CI_COMMIT_BRANCH == "develop"
  needs:
    - build-wheel
    - unit-tests

# ============================================================================
# STAGE 5: DEPLOY STAGING (AUTOMATIC)
# ============================================================================

deploy-staging:
  extends: .databricks-cli
  stage: deploy-staging
  variables:
    DATABRICKS_HOST: $DATABRICKS_HOST_STAGING
    DATABRICKS_TOKEN: $DATABRICKS_TOKEN_STAGING
    CATALOG: $CATALOG_STAGING
  environment:
    name: staging
  script:
    - |
      WHEEL_FILE=$(ls dist/*.whl | head -1)
      WHEEL_NAME=$(basename $WHEEL_FILE)

      databricks fs cp $WHEEL_FILE ${VOLUME_PATH}/${WHEEL_NAME} --overwrite
      databricks workspace import-dir notebooks /Repos/staging/${CI_PROJECT_NAME} --overwrite

      # Deploy jobs with staging suffix
      for job_file in jobs/*.yaml; do
        if [ -f "$job_file" ]; then
          JOB_NAME=$(basename $job_file .yaml)
          echo "Deploying staging job: ${JOB_NAME}_staging"
          # Similar logic as dev
        fi
      done
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
  needs:
    - build-wheel
    - unit-tests

# ============================================================================
# STAGE 6: DEPLOY PROD (MANUAL)
# ============================================================================

deploy-prod:
  extends: .databricks-cli
  stage: deploy-prod
  variables:
    DATABRICKS_HOST: $DATABRICKS_HOST_PROD
    DATABRICKS_TOKEN: $DATABRICKS_TOKEN_PROD
    CATALOG: $CATALOG_PROD
  environment:
    name: production
  script:
    - |
      WHEEL_FILE=$(ls dist/*.whl | head -1)
      WHEEL_NAME=$(basename $WHEEL_FILE)

      # Versioned wheel upload (never overwrite in prod)
      VERSION=$(echo $WHEEL_NAME | grep -oP '\d+\.\d+\.\d+')
      databricks fs cp $WHEEL_FILE ${VOLUME_PATH}/${WHEEL_NAME}

      # Update jobs to use new wheel version
      databricks workspace import-dir notebooks /Repos/prod/${CI_PROJECT_NAME} --overwrite

      echo "Deployed version $VERSION to production"
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: manual
  needs:
    - deploy-staging

# ============================================================================
# DATA QUALITY GATES (Lesson #88-95)
# ============================================================================

# Run after notebooks complete (triggered by job completion webhook)
data-quality-check:
  extends: .databricks-cli
  stage: .post
  variables:
    DATABRICKS_HOST: $DATABRICKS_HOST_DEV
    DATABRICKS_TOKEN: $DATABRICKS_TOKEN_DEV
  script:
    - |
      echo "Running data quality checks..."
      # This would typically be triggered by job completion
      # For now, validate table existence and row counts
      databricks sql execute --sql "
        SELECT table_name,
               (SELECT COUNT(*) FROM \${table_name}) as row_count
        FROM information_schema.tables
        WHERE table_schema = 'silver'
      "
  when: manual
  allow_failure: true
